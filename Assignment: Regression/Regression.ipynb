{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcyrpibUIAPTMU6uwJVdg9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rgarlay/PWSkills_Assignments/blob/main/Assignment%3A%20Regression/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. What is Simple Linear Regression?"
      ],
      "metadata": {
        "id": "jH8AzFCYhjeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression is a machine learning algorithm that is used to estimate a variable by other variable.\n",
        "\n",
        "Simple implies that only one independent variable(x) or feature is present to determine target feature 'y'.\n",
        "\n",
        "Linear implies linearity of 2 variable relationship. Algorithm assumes that both variable are linearly related.\n",
        "\n",
        "Regression means to measure the dependent variable from other variable."
      ],
      "metadata": {
        "id": "pMWSnrVlhp0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1a-gMEqgAsc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. What are the key assumptions of Simple Linear Regression?\n"
      ],
      "metadata": {
        "id": "j5CqBDZHiodt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Both variables must be linearly related.\n",
        "\n",
        "2. The errors must be normally distributed.\n",
        "\n",
        "3. Homoscedascity i.e. constant variance of errors."
      ],
      "metadata": {
        "id": "0DIdzrvziqTM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuzySGbmiprM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. What does the coefficient `m` represent in the equation `Y=mX+c`?"
      ],
      "metadata": {
        "id": "5gg2ecdBkGIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It represents slope.\n",
        "It machine learning terminology, it represents weight of the 'x' feature for determining target feature 'y'."
      ],
      "metadata": {
        "id": "2V4-7dKykK5X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D0WmmmO0kIXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. What does the intercept `c` represent in the equation `Y=mX+c` ?"
      ],
      "metadata": {
        "id": "bL_jhHpTkYSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It represents intercepts. It gives value of Y meets the x axis."
      ],
      "metadata": {
        "id": "G6_HIiqBkba6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QrJEblrskauA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. How do we calculate the slope m in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "FI65vxH9krZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For scientific calculation purposes ,we can use method of least square to find the value of m.\n",
        "\n",
        "m = `[n * sum(x*y) - sum(x)*sum(y)]/[n*sum(x**2) - (sum(x)**2)]`\n",
        "\n",
        "Further optimization in its value is done by optimization algorithm, namely gradient descent.\n"
      ],
      "metadata": {
        "id": "Jaj7aMt-kuIi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2v6gTz4kttw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. What is the purpose of the least squares method in Simple Linear Regression?\n"
      ],
      "metadata": {
        "id": "gK2110Wel-wK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Least square method takes the difference between predicted 'y' and actual 'y' (target), squares it and tries to minimise that difference.\n",
        "\n",
        "This becomes a basis for starting to calculate slope and intercept value and gives us formula for doing so."
      ],
      "metadata": {
        "id": "cG7d9lN8mBI8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60t7OPt-mAs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "YtUWnlA0mpZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R_squared quantity calculates the goodness of fit of the predicted line over training data.\n",
        "\n",
        "Ranging between 0 to 1, it is a metric to quantify how well our current model fits onto the training data.\n",
        "\n",
        "High value of R_square means that almost all of the total variance is close explained variance and thus captured by model.\n",
        "\n"
      ],
      "metadata": {
        "id": "WnVyIUJGmslK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKDUA24Bmqjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. What is Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "gCLCNCX9nosy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression is almost same as simple linear regression, however, we have multiple independent feature.\n",
        "a\n",
        "For example, if target variable is 'price of house', then training feauters can be 'length of land','width of land', 'age of the house'."
      ],
      "metadata": {
        "id": "YbK_z2CnnqcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbrsmcPPnp7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9. What is the main difference between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "uBY0-nS7oOFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple linear regression ,we have one feature and one target variable.\n",
        "It is simple and visualisable in 2D geaph. Feature-wise, it is computationally cheap.\n",
        "\n",
        "In multiple linear regression, we have 2 or more trainig features for predicting target variable.\n",
        "It is in multiple dimensions and we cannot visualise it entirely.\n",
        "It can be computationally expensive."
      ],
      "metadata": {
        "id": "WLu7mjS-oPlk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "al5a4wLloPJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. What are the key assumptions of Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "uTiXeSxYovfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Both variables must be linearly related.\n",
        "\n",
        "2. The errors must be normally distributed.\n",
        "\n",
        "3. Homoscedascity i.e. constant variance of errors.\n",
        "\n",
        "4. Absence of multicolinearity (especially for multiple linear regression features)."
      ],
      "metadata": {
        "id": "hlOfXtpbox5b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrZ92E9WoxLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
      ],
      "metadata": {
        "id": "TZ2r75xXo7s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasicity means that the variance of the error of features is not constant.\n",
        "\n",
        "This is problematic in linear regression, beacuase this could be an indication that the data is not entirely or sufficently independent.\n",
        "\n",
        "This could lead to features influencing each other and affecting each other's weight coefficent, ultimately leading to a model with bad fit on training data."
      ],
      "metadata": {
        "id": "32nFdmTpsTAX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QJS_fFq0o9uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12. - How can you improve a Multiple Linear Regression model with high multicollinearity?"
      ],
      "metadata": {
        "id": "FqGx6CPGtF6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deal with multicolinearity, we can do feature engineering or extraction.\n",
        "\n",
        "We can try to engineer feature from present feature which if more effective as predictor and remove its parent feature.\n",
        "\n",
        "Or, from the exisitng pool of features, we can delete from some features which are mutually correlated."
      ],
      "metadata": {
        "id": "blshyJJKtIVy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbAMQXJCtHz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. What are some common techniques for transforming categorical variables for use in regression models."
      ],
      "metadata": {
        "id": "-tSPnr8BuwMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use categorical feature in regression model:\n",
        "\n",
        "OneHotEncoding:- To transform nominal variable(say 'x_i') and assign a different array/column where the value is 1 or 0, depending on whether we are at that variable 'x_i' or not.\n",
        "\n",
        "OrdinalEncoder:- To transform ordinal data where the data is ranked and internal order is preserved."
      ],
      "metadata": {
        "id": "5gxisEwQu2nj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYTc7HtAVksc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14. E- What is the role of interaction terms in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "rzJnyHBHVj-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interaction terms are the variable, that are the result of combining 2 already-present independent variable.\n",
        "\n",
        "This is a great method to check whether the correlation of the old varble changes with dependent variable, when other feature interacts with it.\n",
        "This also identifies and captures the internal dependence of these features."
      ],
      "metadata": {
        "id": "3wcjsoD5VnrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ruruwdNwu1Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression."
      ],
      "metadata": {
        "id": "sqiMn0b_WcBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Simple regression, the governing equation is:-\n",
        "    `y = mx+c`\n",
        "\n",
        "`c` is intercept. The line describing 'x' and 'y' relation is 'c' when it meets the x axis.\n",
        "This is a 1-D line, that exists on 2D plane.\n",
        "\n",
        "In Multiple linear regression, there are multiple weights i.e. `m`, hence our equation becomes multidimensional equvalently.\n",
        "\n",
        "y = (m_1)x_1 + (m_2)x_2 .....(m_i)x_i + c\n",
        "\n",
        "where c is the additioon of intercept of all.\n",
        "When all of the weights are zero, then the axis crosses.\n",
        "However, in practical experience, all of the weights are not zero simultaneously."
      ],
      "metadata": {
        "id": "3qRd22TDW9Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "irH2hRIWWeNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. What is the significance of the slope in regression analysis, and how does it affect predictions?"
      ],
      "metadata": {
        "id": "hqvyIsHIYvOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In linear regression analysis, we only consider linearity of deature and target, Slope is an important measure.\n",
        "\n",
        "ALso interpreted as weight of feature, slope determine how much importance each feature has.\n",
        "\n",
        "Slope radically changes the prediction as error parameter can be reduced by optimiing it.\n",
        "If the slope of the predicted line minimses the error/loss function, then we consider that the bestfit line."
      ],
      "metadata": {
        "id": "paDYDZ3QY1pW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RbUOvheY1H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. How does the intercept in a regression model provide context for the relationship between variables?"
      ],
      "metadata": {
        "id": "YGHhlegGaO5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intercept introduces a baseline value to the regression analysis.\n",
        "It is the value of target variable when coefficent of training feature variable is zero.\n",
        "\n",
        "It helps compare regression models.\n",
        "For example, if adding variables reduce intercept while improving prediction accuracy, it will hint that there were previous unaccounted factors in baseline estimate.\n",
        "\n",
        "It also helps in predicting the quality of model.\n",
        "For example, if the regression model of employ salary has negative baseline salary value i.e. intercept, then it would signal bad quality as it is not fesible.\n"
      ],
      "metadata": {
        "id": "sJgtxaQ7aRqG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtqgodFLaRDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18. What are the limitations of using R² as a sole measure of model performance?"
      ],
      "metadata": {
        "id": "mX19InGybzWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> It does not indicate model accuracy.\n",
        "\n",
        "> It does not detect overfitting.\n",
        "\n",
        "> It does not detect non-linear relations.\n",
        "\n",
        "> It t is sensitive to data range and scale."
      ],
      "metadata": {
        "id": "v5-ZcCiqb-z_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hKIi39SWb0yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19. How would you interpret a large standard error for a regression coefficient?"
      ],
      "metadata": {
        "id": "KqbSSwqGdu9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " β0 and  β1 are regression coefficent in simple linear regression.\n",
        "\n",
        " Standard error gives the measure of variation of the calculated mean and the actual mean of the population.\n",
        "\n",
        " If this quantity is large, then we can assert that some factors are unaccounted and model is actually missing its mark from understanding the patterns that are in the data.\n",
        "\n",
        " To eradicate this, we can try adding more data to the training samples."
      ],
      "metadata": {
        "id": "uph2P5M0dxZd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUeIIGMNdwsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
      ],
      "metadata": {
        "id": "9eWNsvhGxGKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The constant variance of the error across the data is called  `Homoscedasticiy`.\n",
        "\n",
        "On contrary, if the errors are distributed rather randomly and their variance is not the same, then we can call it `Heteroscadasicity`.\n",
        "\n",
        "Residual/scatter plot can display the location of these residuals and thus assist us in visually understanding whether the errors are evenly distributed w.r.t the mean.\n",
        "\n",
        "It is important to address this, as Homoscadasticity is an important assumtion in linear models. Modeling despite it, can be inaccurate and miss some important information."
      ],
      "metadata": {
        "id": "DaiQEXHkxIe8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzqLKCzxxH8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?"
      ],
      "metadata": {
        "id": "ndZRq8pryTza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It means that the number of features that we've added into our model do not increase the accuracy of the model considerably, however they are making our model more complex, making it harder to capture all the underlying patterns. This can be computationally expensive while being very low effective.\n",
        "\n"
      ],
      "metadata": {
        "id": "sZIYSP17yVym"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lt1SkTnyyVYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22. Why is it important to scale variables in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "s5q0Qo-eypQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling variable makes sure that all that there is no bias toward any feature due to its order of magnitude or the variable with lower order values in its raw is not overlooked.\n",
        "\n",
        "Since, the scale of data in linear regression can be different. This is important step to make sure data does not overweigh or underweigh any feature.\n"
      ],
      "metadata": {
        "id": "JkOOuTm0ysvn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8uPx2bpysKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. What is polynomial regression?"
      ],
      "metadata": {
        "id": "wVADiIMVzLrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a machine learning method that is used for modeling non-linear data.\n",
        "\n",
        "`y = β0 + β1*x1 + β2*x2.....βn * xn`\n",
        "\n",
        "y = target variable\n",
        "β = weight coefficents\n",
        "x = independent features.\n",
        "\n",
        "This is important, since linear regression can only be effectively applied to linear data.\n"
      ],
      "metadata": {
        "id": "v343t_przNa-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r98a6b3-zM1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24.  How does polynomial regression differ from linear regression?"
      ],
      "metadata": {
        "id": "ecDVv_74z5NO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is ideal for linear data. It assumes linearity of the independent and dependent feature relation. It is easier to understand, interpret and is less prone to overfitting.\n",
        "\n",
        "Polynomial regression is used for learning non-linear feature. It has higher degree of complexity, is prone to overfitting. However, it offers greater flexiability to model complex patterns."
      ],
      "metadata": {
        "id": "Iyr03FIAz701"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pH-zDa_z7Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25. When is polynomial regression used?\n"
      ],
      "metadata": {
        "id": "xyMbKiby0l1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the relatioship between dependent and independent variable is non-linear."
      ],
      "metadata": {
        "id": "56DO7_D50nvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BN7PR2qO0nU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###26. What is the general equation for polynomial regression?"
      ],
      "metadata": {
        "id": "MkYtvudC0uNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`y = β0 + β1*x1 + β2*x2.....βn * xn`"
      ],
      "metadata": {
        "id": "Mo_yYcO-0zEf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNdhq8t50vhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###27. - Can polynomial regression be applied to multiple variables?"
      ],
      "metadata": {
        "id": "78jXY2GT01mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can be used to model multiple independent features with target variable."
      ],
      "metadata": {
        "id": "bLFAsbnx05Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DsQngDRI03MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###28. What are the limitations of polynomial regression?"
      ],
      "metadata": {
        "id": "AkfOTn5z1CtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is prone to overfitting.\n",
        "\n",
        "It is difficult to interpret.\n",
        "\n",
        "Beyond few degrees of polynomial, it can be computationaly expensive and time consuming.\n",
        "\n",
        "It does not work well with small amount of data. It needs large amount of data to model polynomial relation correctly."
      ],
      "metadata": {
        "id": "g77hS2K61GLD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLNgc3M51EZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
      ],
      "metadata": {
        "id": "XRcmypDG1jwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a test data set and predicting the target variable.\n",
        "\n",
        "Then it can be specaluated using evaluation metrics like: MSR, RMSE, MAE."
      ],
      "metadata": {
        "id": "kAKsvmzU1lWC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4S7ah3i1k2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###30.  Why is visualization important in polynomial regression?"
      ],
      "metadata": {
        "id": "WHT6xNvI2ME1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because Visualization is a great way of inspecting for overfitting and underfitting.\n",
        "\n",
        "By looking at how the model line models through data, an approximate understanding whether model is too simple or complex can be drawn."
      ],
      "metadata": {
        "id": "pMhjs92W2QK5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ej8QsKJu2NOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###31. How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "lMVBxAqw2ihX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement Polynomial regression ,we can import linear_models.\n",
        "\n",
        "Then we can import the features using `sklearn` modeule `preprocessing` and function `PolynomialFeatures`.\n",
        "\n",
        "Then we create an object with desired degree of polynomial and transform our data into polynomial features.\n",
        "\n",
        "Further, we can fit the new object onto our training and target feature and use test dataset to confirm its performance.\n"
      ],
      "metadata": {
        "id": "OYyokiYg2kUu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsCiH46L2jwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}